{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 6.747541189193726,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.9",
  "scores": {
    "test": [
      {
        "accuracy": 0.552685546875,
        "ap": 0.5300021723503934,
        "ap_weighted": 0.5300021723503934,
        "f1": 0.5481667517087125,
        "f1_weighted": 0.5481667517087125,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.552685546875,
        "scores_per_experiment": [
          {
            "accuracy": 0.56201171875,
            "ap": 0.5355372105149597,
            "ap_weighted": 0.5355372105149597,
            "f1": 0.5594884658796211,
            "f1_weighted": 0.5594884658796211
          },
          {
            "accuracy": 0.60546875,
            "ap": 0.5626049339254766,
            "ap_weighted": 0.5626049339254766,
            "f1": 0.6038726443504148,
            "f1_weighted": 0.6038726443504148
          },
          {
            "accuracy": 0.53173828125,
            "ap": 0.5166559401220443,
            "ap_weighted": 0.5166559401220443,
            "f1": 0.522358208483354,
            "f1_weighted": 0.522358208483354
          },
          {
            "accuracy": 0.5390625,
            "ap": 0.5219957610410095,
            "ap_weighted": 0.5219957610410095,
            "f1": 0.5217183496923955,
            "f1_weighted": 0.5217183496923955
          },
          {
            "accuracy": 0.53662109375,
            "ap": 0.5195951970299345,
            "ap_weighted": 0.5195951970299345,
            "f1": 0.536397267452858,
            "f1_weighted": 0.536397267452858
          },
          {
            "accuracy": 0.50390625,
            "ap": 0.501965705515298,
            "ap_weighted": 0.501965705515298,
            "f1": 0.49822078817781507,
            "f1_weighted": 0.49822078817781507
          },
          {
            "accuracy": 0.5966796875,
            "ap": 0.5563559987437185,
            "ap_weighted": 0.5563559987437185,
            "f1": 0.5938814098224325,
            "f1_weighted": 0.5938814098224325
          },
          {
            "accuracy": 0.52294921875,
            "ap": 0.5119565634774352,
            "ap_weighted": 0.5119565634774352,
            "f1": 0.5219205219054691,
            "f1_weighted": 0.5219205219054691
          },
          {
            "accuracy": 0.5673828125,
            "ap": 0.5393338990442962,
            "ap_weighted": 0.5393338990442962,
            "f1": 0.5632173283418835,
            "f1_weighted": 0.5632173283418835
          },
          {
            "accuracy": 0.56103515625,
            "ap": 0.5340205140897613,
            "ap_weighted": 0.5340205140897613,
            "f1": 0.5605925329808817,
            "f1_weighted": 0.5605925329808817
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}